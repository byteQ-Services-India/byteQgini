üß† byteQgini ‚Äî Offline Document Intelligence Engine (Fast Local RAG)
====================================================================

![](https://img.shields.io/badge/Build-Stable-00C853?style=for-the-badge) ![](https://img.shields.io/badge/Version-1.1.0-2962FF?style=for-the-badge) ![](https://img.shields.io/badge/Runtime-CPU_Only-blue?style=for-the-badge) ![](https://img.shields.io/badge/Vector_DB-FAISS_HNSW-orange?style=for-the-badge) ![](https://img.shields.io/badge/Embeddings-MiniLM--L6--v2-purple?style=for-the-badge) ![](https://img.shields.io/badge/LLM-llama.cpp_GGUF-red?style=for-the-badge) ![](https://img.shields.io/badge/Privacy-100%_Local-green?style=for-the-badge) ![](https://img.shields.io/badge/License-Open_Source-green?style=for-the-badge)

üöÄ Overview
-----------

**byteQgini** is a **fully offline, privacy-first Retrieval-Augmented Generation (RAG) system** designed for **fast, deterministic document intelligence on CPU**.

It ingests PDF documents, builds a persistent semantic index, and answers user queries **strictly from the provided documents** ‚Äî all **without cloud APIs, external services, or data leakage**.

The system is optimized to achieve **~1‚Äì3 second response latency on CPU** and is designed to be **downloadable, open-source, and production-ready**.

üéØ Key Design Goals
-------------------

*   üîí **100% Local & Private** ‚Äî no external APIs
    
*   ‚ö° **Low-latency (1‚Äì3s SLA)** on CPU
    
*   üß† **Strict document grounding** (no hallucinations)
    
*   üì¶ **Downloadable / sellable offline bundle**
    
*   üõ†Ô∏è **Deterministic & stable architecture**
    
*   üìà **Scales with growing document sets**
    

üß© What byteQgini Delivers
--------------------------

*   Automatic ingestion of PDFs from the ./data/ directory
    
*   Deterministic chunking of documents using RecursiveCharacterTextSplitter
    
*   High-quality semantic embeddings via sentence-transformers/all-MiniLM-L6-v2
    
*   **FAISS HNSW** index for fast approximate nearest-neighbor search
    
*   Persistent storage of embeddings and document chunks
    
*   Retrieval-augmented response generation using **llama.cpp (GGUF models)**
    
*   **Static dataset memory + KV-cache reuse** for low latency
    
*   Lightweight Flask HTTP interface for UI or programmatic access
    

üß† High-Level Architecture
--------------------------

Plain textANTLR4BashCC#CSSCoffeeScriptCMakeDartDjangoDockerEJSErlangGitGoGraphQLGroovyHTMLJavaJavaScriptJSONJSXKotlinLaTeXLessLuaMakefileMarkdownMATLABMarkupObjective-CPerlPHPPowerShell.propertiesProtocol BuffersPythonRRubySass (Sass)Sass (Scss)SchemeSQLShellSwiftSVGTSXTypeScriptWebAssemblyYAMLXML`   PDFs (static)     ‚Üì  Chunking + Embeddings (once)     ‚Üì  FAISS HNSW (fast ANN search)     ‚Üì  Relevant document chunk     ‚Üì  llama.cpp (cached system + dataset memory)     ‚Üì  Answer (offline, private, fast)   `

‚öôÔ∏è How It Works
---------------

### 1Ô∏è‚É£ Startup Initialization

*   Loads existing FAISS index and document store if available
    
*   Otherwise:
    
    *   Reads all PDFs from ./data/
        
    *   Chunks and embeds text
        
    *   Builds a FAISS HNSW index
        
*   Builds a **static dataset summary once**
    
*   Pre-warms the LLM to initialize the **KV cache**
    

### 2Ô∏è‚É£ Retrieval (Fast Path)

*   User query is embedded once
    
*   FAISS HNSW retrieves the most relevant chunk (k = 1)
    
*   Out-of-scope queries are hard-blocked
    

### 3Ô∏è‚É£ Answer Generation (Optimized)

*   Static prompt prefix (system rules + dataset summary) is **cached**
    
*   Only the **context chunk + question** are processed per request
    
*   llama.cpp generates a concise, factual answer
    
*   Typical latency: **~1‚Äì3 seconds on CPU**
    

üìÅ Project Structure
--------------------

Plain textANTLR4BashCC#CSSCoffeeScriptCMakeDartDjangoDockerEJSErlangGitGoGraphQLGroovyHTMLJavaJavaScriptJSONJSXKotlinLaTeXLessLuaMakefileMarkdownMATLABMarkupObjective-CPerlPHPPowerShell.propertiesProtocol BuffersPythonRRubySass (Sass)Sass (Scss)SchemeSQLShellSwiftSVGTSXTypeScriptWebAssemblyYAMLXML`   byteQgini/  ‚îú‚îÄ app.py                    # Core application (Flask + RAG engine)  ‚îú‚îÄ data/                     # Place your PDF files here  ‚îú‚îÄ models/  ‚îÇ   ‚îî‚îÄ llama.gguf            # GGUF model for llama.cpp  ‚îú‚îÄ precomputed_data/  ‚îÇ   ‚îú‚îÄ index.faiss           # FAISS HNSW index  ‚îÇ   ‚îî‚îÄ docs.pkl              # Serialized document chunks  ‚îú‚îÄ templates/  ‚îÇ   ‚îî‚îÄ index.html            # Chat UI  ‚îú‚îÄ requirements.txt  ‚îî‚îÄ README.md   `

üîå API Endpoints
----------------

### GET /

*   Serves the chat UI (index.html)
    

### GET /get?msg=your+question

*   Returns a plain-text response grounded in documents
    

Plain textANTLR4BashCC#CSSCoffeeScriptCMakeDartDjangoDockerEJSErlangGitGoGraphQLGroovyHTMLJavaJavaScriptJSONJSXKotlinLaTeXLessLuaMakefileMarkdownMATLABMarkupObjective-CPerlPHPPowerShell.propertiesProtocol BuffersPythonRRubySass (Sass)Sass (Scss)SchemeSQLShellSwiftSVGTSXTypeScriptWebAssemblyYAMLXML`   curl "http://127.0.0.1:5000/get?msg=what is covered in these documents"   `

### POST /get

*   Accepts msg as form data
    
*   Same behavior as GET
    

üß™ Installation & Setup
-----------------------

### 1Ô∏è‚É£ Clone Repository

Plain textANTLR4BashCC#CSSCoffeeScriptCMakeDartDjangoDockerEJSErlangGitGoGraphQLGroovyHTMLJavaJavaScriptJSONJSXKotlinLaTeXLessLuaMakefileMarkdownMATLABMarkupObjective-CPerlPHPPowerShell.propertiesProtocol BuffersPythonRRubySass (Sass)Sass (Scss)SchemeSQLShellSwiftSVGTSXTypeScriptWebAssemblyYAMLXML`   git clone https://github.com/byteQ-services/byteQgini.git  cd byteQgini   `

### 2Ô∏è‚É£ Create Virtual Environment

Plain textANTLR4BashCC#CSSCoffeeScriptCMakeDartDjangoDockerEJSErlangGitGoGraphQLGroovyHTMLJavaJavaScriptJSONJSXKotlinLaTeXLessLuaMakefileMarkdownMATLABMarkupObjective-CPerlPHPPowerShell.propertiesProtocol BuffersPythonRRubySass (Sass)Sass (Scss)SchemeSQLShellSwiftSVGTSXTypeScriptWebAssemblyYAMLXML`   python3.11 -m venv .venv  source .venv/bin/activate   `

### 3Ô∏è‚É£ Install Dependencies

Plain textANTLR4BashCC#CSSCoffeeScriptCMakeDartDjangoDockerEJSErlangGitGoGraphQLGroovyHTMLJavaJavaScriptJSONJSXKotlinLaTeXLessLuaMakefileMarkdownMATLABMarkupObjective-CPerlPHPPowerShell.propertiesProtocol BuffersPythonRRubySass (Sass)Sass (Scss)SchemeSQLShellSwiftSVGTSXTypeScriptWebAssemblyYAMLXML`   pip install -r requirements.txt   `

### 4Ô∏è‚É£ Download GGUF Model

Plain textANTLR4BashCC#CSSCoffeeScriptCMakeDartDjangoDockerEJSErlangGitGoGraphQLGroovyHTMLJavaJavaScriptJSONJSXKotlinLaTeXLessLuaMakefileMarkdownMATLABMarkupObjective-CPerlPHPPowerShell.propertiesProtocol BuffersPythonRRubySass (Sass)Sass (Scss)SchemeSQLShellSwiftSVGTSXTypeScriptWebAssemblyYAMLXML`   mkdir -p models  wget -O models/llama.gguf \  https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf   `

### 5Ô∏è‚É£ Run Application

Plain textANTLR4BashCC#CSSCoffeeScriptCMakeDartDjangoDockerEJSErlangGitGoGraphQLGroovyHTMLJavaJavaScriptJSONJSXKotlinLaTeXLessLuaMakefileMarkdownMATLABMarkupObjective-CPerlPHPPowerShell.propertiesProtocol BuffersPythonRRubySass (Sass)Sass (Scss)SchemeSQLShellSwiftSVGTSXTypeScriptWebAssemblyYAMLXML`   python app.py   `

Open:

Plain textANTLR4BashCC#CSSCoffeeScriptCMakeDartDjangoDockerEJSErlangGitGoGraphQLGroovyHTMLJavaJavaScriptJSONJSXKotlinLaTeXLessLuaMakefileMarkdownMATLABMarkupObjective-CPerlPHPPowerShell.propertiesProtocol BuffersPythonRRubySass (Sass)Sass (Scss)SchemeSQLShellSwiftSVGTSXTypeScriptWebAssemblyYAMLXML`   http://127.0.0.1:5000   `

üß∞ Tech Stack
-------------

### Core

*   Python **3.10 ‚Äì 3.11** (recommended)
    
*   Flask
    
*   FAISS (CPU, HNSW)
    
*   HuggingFace sentence-transformers
    
*   llama.cpp (llama-cpp-python)
    
*   NumPy
    

### Models

*   Embeddings: all-MiniLM-L6-v2
    
*   LLM: GGUF-quantized LLaMA / compatible instruct models
    

‚ö†Ô∏è Known Constraints
--------------------

*   Single-chunk retrieval (k = 1) by design (for latency)
    
*   No authentication or rate limiting (local use only)
    
*   CPU-only inference (GPU not required)
    

üîÆ Roadmap
----------

*   Multi-chunk aggregation
    
*   Confidence / similarity scoring
    
*   Response caching
    
*   Desktop & CLI packaging
    
*   SLA benchmarks & telemetry
    
*   Domain-specific bundles (legal, medical, finance)
    

üèÜ Why This Project Matters
---------------------------

Most RAG systems:

*   depend on cloud APIs
    
*   have unpredictable latency
    
*   leak data by design
    

**byteQgini** is different:

*   fully offline
    
*   deterministic
    
*   fast
    
*   and genuinely shippable
    

ü§ù Contributions
----------------

Issues, discussions, and pull requests are welcome.This project is built to be **extended, audited, and trusted**.
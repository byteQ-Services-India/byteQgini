ğŸ§  byteQgini â€” Offline Document Intelligence Engine (Fast Local RAG)
====================================================================

![](https://img.shields.io/badge/Build-Stable-00C853?style=for-the-badge) ![](https://img.shields.io/badge/Version-1.1.0-2962FF?style=for-the-badge) ![](https://img.shields.io/badge/Runtime-CPU_Only-blue?style=for-the-badge) ![](https://img.shields.io/badge/Vector_DB-FAISS_HNSW-orange?style=for-the-badge) ![](https://img.shields.io/badge/Embeddings-MiniLM--L6--v2-purple?style=for-the-badge) ![](https://img.shields.io/badge/LLM-llama.cpp_GGUF-red?style=for-the-badge) ![](https://img.shields.io/badge/Privacy-100%_Local-green?style=for-the-badge) ![](https://img.shields.io/badge/License-Open_Source-green?style=for-the-badge)

ğŸš€ Overview
-----------

**byteQgini** is a **fully offline, privacy-first Retrieval-Augmented Generation (RAG) system** designed for **fast, deterministic document intelligence on CPU**.

It ingests PDF documents, builds a persistent semantic index, and answers user queries **strictly from the provided documents** â€” all **without cloud APIs, external services, or data leakage**.

The system is optimized to achieve **~1â€“3 second response latency on CPU** and is designed to be **downloadable, open-source, and production-ready**.

ğŸ¯ Key Design Goals
-------------------

*   ğŸ”’ **100% Local & Private** â€” no external APIs
    
*   âš¡ **Low-latency (1â€“3s SLA)** on CPU
    
*   ğŸ§  **Strict document grounding** (no hallucinations)
    
*   ğŸ“¦ **Downloadable / sellable offline bundle**
    
*   ğŸ› ï¸ **Deterministic & stable architecture**
    
*   ğŸ“ˆ **Scales with growing document sets**
    

ğŸ§© What byteQgini Delivers
--------------------------

*   Automatic ingestion of PDFs from the ./data/ directory
    
*   Deterministic chunking of documents using RecursiveCharacterTextSplitter
    
*   High-quality semantic embeddings via sentence-transformers/all-MiniLM-L6-v2
    
*   **FAISS HNSW** index for fast approximate nearest-neighbor search
    
*   Persistent storage of embeddings and document chunks
    
*   Retrieval-augmented response generation using **llama.cpp (GGUF models)**
    
*   **Static dataset memory + KV-cache reuse** for low latency
    
*   Lightweight Flask HTTP interface for UI or programmatic access
    

ğŸ§  High-Level Architecture
--------------------------
   PDFs (static)     â†“  Chunking + Embeddings (once)     â†“  FAISS HNSW (fast ANN search)     â†“  Relevant document chunk     â†“  llama.cpp (cached system + dataset memory)     â†“  Answer (offline, private, fast)   `

âš™ï¸ How It Works
---------------

### 1ï¸âƒ£ Startup Initialization

*   Loads existing FAISS index and document store if available
    
*   Otherwise:
    
    *   Reads all PDFs from ./data/
        
    *   Chunks and embeds text
        
    *   Builds a FAISS HNSW index
        
*   Builds a **static dataset summary once**
    
*   Pre-warms the LLM to initialize the **KV cache**
    

### 2ï¸âƒ£ Retrieval (Fast Path)

*   User query is embedded once
    
*   FAISS HNSW retrieves the most relevant chunk (k = 1)
    
*   Out-of-scope queries are hard-blocked
    

### 3ï¸âƒ£ Answer Generation (Optimized)

*   Static prompt prefix (system rules + dataset summary) is **cached**
    
*   Only the **context chunk + question** are processed per request
    
*   llama.cpp generates a concise, factual answer
    
*   Typical latency: **~1â€“3 seconds on CPU**
    

ğŸ“ Project Structure
--------------------

`   byteQgini/  â”œâ”€ app.py                    # Core application (Flask + RAG engine)  â”œâ”€ data/                     # Place your PDF files here  â”œâ”€ models/  â”‚   â””â”€ llama.gguf            # GGUF model for llama.cpp  â”œâ”€ precomputed_data/  â”‚   â”œâ”€ index.faiss           # FAISS HNSW index  â”‚   â””â”€ docs.pkl              # Serialized document chunks  â”œâ”€ templates/  â”‚   â””â”€ index.html            # Chat UI  â”œâ”€ requirements.txt  â””â”€ README.md   `

ğŸ”Œ API Endpoints
----------------

### GET /

*   Serves the chat UI (index.html)
    

### GET /get?msg=your+question

*   Returns a plain-text response grounded in documents
    

`   curl "http://127.0.0.1:5000/get?msg=what is covered in these documents"   `

### POST /get

*   Accepts msg as form data
    
*   Same behavior as GET
    

ğŸ§ª Installation & Setup
-----------------------

### 1ï¸âƒ£ Clone Repository

`   git clone https://github.com/byteQ-services/byteQgini.git  cd byteQgini   `

### 2ï¸âƒ£ Create Virtual Environment

`   python3.11 -m venv .venv  source .venv/bin/activate   `

### 3ï¸âƒ£ Install Dependencies

`   pip install -r requirements.txt   `

### 4ï¸âƒ£ Download GGUF Model

`   mkdir -p models  wget -O models/llama.gguf \  https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf   `

### 5ï¸âƒ£ Run Application

`   python app.py   `

Open:

`   http://127.0.0.1:5000   `

ğŸ§° Tech Stack
-------------

### Core

*   Python **3.10 â€“ 3.11** (recommended)
    
*   Flask
    
*   FAISS (CPU, HNSW)
    
*   HuggingFace sentence-transformers
    
*   llama.cpp (llama-cpp-python)
    
*   NumPy
    

### Models

*   Embeddings: all-MiniLM-L6-v2
    
*   LLM: GGUF-quantized LLaMA / compatible instruct models
    

âš ï¸ Known Constraints
--------------------

*   Single-chunk retrieval (k = 1) by design (for latency)
    
*   No authentication or rate limiting (local use only)
    
*   CPU-only inference (GPU not required)
    

ğŸ”® Roadmap
----------

*   Multi-chunk aggregation
    
*   Confidence / similarity scoring
    
*   Response caching
    
*   Desktop & CLI packaging
    
*   SLA benchmarks & telemetry
    
*   Domain-specific bundles (legal, medical, finance)
    

ğŸ† Why This Project Matters
---------------------------

Most RAG systems:

*   depend on cloud APIs
    
*   have unpredictable latency
    
*   leak data by design
    

**byteQgini** is different:

*   fully offline
    
*   deterministic
    
*   fast
    
*   and genuinely shippable
    

ğŸ¤ Contributions
----------------

Issues, discussions, and pull requests are welcome.This project is built to be **extended, audited, and trusted**.
